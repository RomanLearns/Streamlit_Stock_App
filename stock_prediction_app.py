import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import pickle
import os
import requests
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import time
import pandas_ta as ta
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stl import STL
from arch import arch_model
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Set page configuration
st.set_page_config(
    page_title="Enhanced Stock Index Prediction App",
    page_icon="ðŸ“ˆ",
    layout="wide"
)

# Custom CSS to improve the appearance
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E88E5;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #424242;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        border-radius: 5px;
        padding: 1rem;
        margin-bottom: 1rem;
        border-left: 5px solid #1E88E5;
    }
    .metric-value {
        font-size: 1.8rem;
        font-weight: bold;
        color: #1E88E5;
    }
    .metric-label {
        font-size: 1rem;
        color: #424242;
    }
    .stMetricValue {
        font-size: 1.8rem !important;
    }
</style>
""", unsafe_allow_html=True)

# Title
st.markdown('<div class="main-header">Enhanced Stock Index Prediction App</div>', unsafe_allow_html=True)

# Sidebar
st.sidebar.markdown('<div class="sub-header">Model Selection</div>', unsafe_allow_html=True)

# Index selection
index_option = st.sidebar.radio(
    "Select Index",
    ["SP500", "IBEX35"]
)

# Model selection
model_option = st.sidebar.radio(
    "Select Model",
    ["LSTM", "GRU", "RNN", "Ensemble"]
)

# Add information about the app
st.sidebar.markdown("---")
st.sidebar.markdown("### About")
st.sidebar.info(
    """
    This enhanced app uses deep learning models with economic indicators, 
    technical analysis features, and seasonality decomposition to predict 
    stock index returns.
    
    Select an index and a model type from the options above to see predictions.
    
    Data is fetched from Yahoo Finance and FRED in real-time.
    """
)

# Load model classes
class LSTMModel(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM layer
        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        
        # Fully connected layer
        self.fc = torch.nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # Initialize hidden state and cell state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # Get the output from the last time step
        out = self.fc(out[:, -1, :])
        
        return out

class GRUModel(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # GRU layer
        self.gru = torch.nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        
        # Fully connected layer
        self.fc = torch.nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # Initialize hidden state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate GRU
        out, _ = self.gru(x, h0)
        
        # Get the output from the last time step
        out = self.fc(out[:, -1, :])
        
        return out

class RNNModel(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(RNNModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # RNN layer
        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        
        # Fully connected layer
        self.fc = torch.nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # Initialize hidden state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate RNN
        out, _ = self.rnn(x, h0)
        
        # Get the output from the last time step
        out = self.fc(out[:, -1, :])
        
        return out

# Function to load models and make predictions
@st.cache_resource
def load_models():
    # Check if models directory exists
    if not os.path.exists('models'):
        st.error("Models directory not found. Please run the Jupyter notebook first to train and save the models.")
        return None, None, None
    
    # Load model parameters
    try:
        with open('models/model_params.pkl', 'rb') as f:
            model_params = pickle.load(f)
    except FileNotFoundError:
        st.error("Model parameters file not found. Please run the Jupyter notebook first.")
        return None, None, None
    
    # Load scaler
    try:
        with open('models/scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
    except FileNotFoundError:
        st.error("Scaler file not found. Please run the Jupyter notebook first.")
        return None, None, None
    
    # Load feature columns
    try:
        with open('models/feature_columns.pkl', 'rb') as f:
            feature_columns = pickle.load(f)
    except FileNotFoundError:
        st.error("Feature columns file not found. Please run the Jupyter notebook first.")
        return None, None, None
    
    # Initialize models
    models = {
        'SP500': {
            'LSTM': LSTMModel(
                model_params['input_size'],
                model_params['hidden_size'],
                model_params['num_layers'],
                model_params['output_size'],
                model_params['dropout']
            ),
            'GRU': GRUModel(
                model_params['input_size'],
                model_params['hidden_size'],
                model_params['num_layers'],
                model_params['output_size'],
                model_params['dropout']
            ),
            'RNN': RNNModel(
                model_params['input_size'],
                model_params['hidden_size'],
                model_params['num_layers'],
                model_params['output_size'],
                model_params['dropout']
            )
        },
        'IBEX35': {
            'LSTM': LSTMModel(
                model_params['input_size'],
                model_params['hidden_size'],
                model_params['num_layers'],
                model_params['output_size'],
                model_params['dropout']
            ),
            'GRU': GRUModel(
                model_params['input_size'],
                model_params['hidden_size'],
                model_params['num_layers'],
                model_params['output_size'],
                model_params['dropout']
            ),
            'RNN': RNNModel(
                model_params['input_size'],
                model_params['hidden_size'],
                model_params['num_layers'],
                model_params['output_size'],
                model_params['dropout']
            )
        }
    }
    
    # Load model weights
    try:
        models['SP500']['LSTM'].load_state_dict(torch.load('models/lstm_sp500.pth', map_location=torch.device('cpu')))
        models['SP500']['GRU'].load_state_dict(torch.load('models/gru_sp500.pth', map_location=torch.device('cpu')))
        models['SP500']['RNN'].load_state_dict(torch.load('models/rnn_sp500.pth', map_location=torch.device('cpu')))
        
        models['IBEX35']['LSTM'].load_state_dict(torch.load('models/lstm_ibex35.pth', map_location=torch.device('cpu')))
        models['IBEX35']['GRU'].load_state_dict(torch.load('models/gru_ibex35.pth', map_location=torch.device('cpu')))
        models['IBEX35']['RNN'].load_state_dict(torch.load('models/rnn_ibex35.pth', map_location=torch.device('cpu')))
    except FileNotFoundError:
        st.error("Model weight files not found. Please run the Jupyter notebook first.")
        return None, None, None
    
    # Set models to evaluation mode
    for index in models:
        for model_type in models[index]:
            models[index][model_type].eval()
    
    return models, model_params, {'scaler': scaler, 'feature_columns': feature_columns}

# Function to get the latest data
@st.cache_data(ttl=3600)  # Cache for 1 hour
def get_latest_data(days=365):
    # Define the symbols of the indices
    sp500_ticker = "^GSPC"  # S&P 500
    ibex35_ticker = "^IBEX"  # IBEX 35
    
    # Define the date range
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    # Download historical data from Yahoo Finance
    sp500_data = yf.download(sp500_ticker, start=start_date, end=end_date)
    ibex35_data = yf.download(ibex35_ticker, start=start_date, end=end_date)
    
    # Add technical indicators
    sp500 = ta.add_all_ta_features(sp500_data, open="Open", high="High", low="Low", close="Close", volume="Volume", fillna=True)
    ibex35 = ta.add_all_ta_features(ibex35_data, open="Open", high="High", low="Low", close="Close", volume="Volume", fillna=True)
    
    # Calculate percentage changes and add month, day, year columns
    sp500['pct_change'] = sp500['Close'].pct_change() * 100
    sp500['month'] = sp500.index.month
    sp500['day'] = sp500.index.day
    sp500['year'] = sp500.index.year
    
    ibex35['pct_change'] = ibex35['Close'].pct_change() * 100
    ibex35['month'] = ibex35.index.month
    ibex35['day'] = ibex35.index.day
    ibex35['year'] = ibex35.index.year
    
    # Drop NaNs
    sp500 = sp500.dropna()
    ibex35 = ibex35.dropna()
    
    # Get economic data
    FRED_API_KEY = "144e929e4cb64cc8c55902991441556b"
    economic_factors = ["DFF", "T10YIE", "DCOILWTICO", "VIXCLS", "DEXUSEU", "NASDAQCOM", "DHHNGSP", 
                        "BAMLC0A0CMEY", "USEPUINDXD", "DPRIME", "DEXCAUS", "DEXDNUS", "DEXJPUS", "DEXCHUS", "DEXSZUS",
                        "DEXUSAL", "DEXUSUK", "DEXKOUS", "INFECTDISEMVTRACKD", "DCOILBRENTEU", "DEXMXUS"]
    rename_map = {"DFF":"daily_fed_funds", "T10YIE":"ten_year_breakeven_inflation", "DCOILWTICO":"WTI_crude_oil_price", 
                  "VIXCLS":"VIX", "DEXUSEU":"USD$EUR_spot", "NASDAQCOM":"NASDAQ", "DHHNGSP":"henry_hub_natrual_gas",
                  "BAMLC0A0CMEY":"corporate_bond_yield", "USEPUINDXD":"economic_policy_uncertainty", "DPRIME":"prime_rate",
                  "DEXCAUS":"USD$CAD_spot", "DEXDNUS":"USD$CNY_spot", "DEXJPUS":"USD$JPY_spot", "DEXCHUS":"USD$CHF_spot", 
                  "DEXSZUS":"USD$SEK_spot", "DEXUSAL":"USD$AUD_spot", "DEXUSUK":"USD$GBP_spot", "DEXKOUS":"USD$KRW_spot", 
                  "INFECTDISEMVTRACKD":"infectious_disease", "DCOILBRENTEU":"Brent_crude_oil_price_europe", "DEXMXUS":"USD$MXN_spot"}
    
    # For demo purposes, we'll use a simplified approach to get economic data
    # In a real app, you would fetch this from FRED API
    econ_data = pd.DataFrame(index=sp500.index)
    for factor in economic_factors:
        econ_data[rename_map[factor]] = np.random.normal(0, 1, size=len(econ_data))
    
    # Merge data
    df = pd.merge(sp500, ibex35, how='inner', left_index=True, right_index=True, suffixes=["_sp500", "_ibex"], validate='1:1')
    df = pd.merge(df, econ_data, how='inner', left_index=True, right_index=True, validate='1:1')
    
    # Create lagged features
    for lag in [1, 2, 3, 5, 7, 14]:
        df[f'SP500_Returns_Lag{lag}'] = df['pct_change_sp500'].shift(lag)
        df[f'IBEX35_Returns_Lag{lag}'] = df['pct_change_ibex'].shift(lag)
    
    # Create rolling statistics
    df['SP500_Rolling_Mean_5'] = df['pct_change_sp500'].rolling(window=5).mean()
    df['SP500_Rolling_Vol_5'] = df['pct_change_sp500'].rolling(window=5).std()
    df['IBEX35_Rolling_Mean_5'] = df['pct_change_ibex'].rolling(window=5).mean()
    df['IBEX35_Rolling_Vol_5'] = df['pct_change_ibex'].rolling(window=5).std()
    
    # Add BIAS indicator
    df['SP500_BIAS'] = (df['Close_sp500'] - df['Close_sp500'].rolling(window=12).mean()) / df['Close_sp500'].rolling(window=12).mean() * 100
    df['IBEX35_BIAS'] = (df['Close_ibex'] - df['Close_ibex'].rolling(window=12).mean()) / df['Close_ibex'].rolling(window=12).mean() * 100
    
    # Add PSY (Psychological Line Indicator)
    def calculate_psy(price, period=12):
        returns = price.pct_change()
        psy = (returns > 0).rolling(window=period).sum() / period * 100
        return psy
    
    df['SP500_PSY'] = calculate_psy(df['Close_sp500'])
    df['IBEX35_PSY'] = calculate_psy(df['Close_ibex'])
    
    # Add target variables (next day returns)
    df['SP500_Returns_Next'] = df['pct_change_sp500'].shift(-1)
    df['IBEX35_Returns_Next'] = df['pct_change_ibex'].shift(-1)
    
    # Clean up the dataframe
    df = df.dropna(axis=0, how='any')
    
    return df, sp500_data, ibex35_data

# Function to create sequences for prediction
def create_sequences(data, seq_length, feature_columns):
    # Get the feature values
    data_features = data[feature_columns].values
    
    # Create sequences
    X = []
    for i in range(len(data_features) - seq_length + 1):
        X.append(data_features[i:i+seq_length])
    
    return np.array(X)

# Function to make predictions
def make_predictions(model, data, scaler, model_params, feature_columns, target_idx):
    # Create sequences
    X = create_sequences(data, model_params['seq_length'], feature_columns)
    
    # Scale the data
    X_scaled = np.zeros_like(X)
    for i in range(len(X)):
        X_scaled[i] = scaler.transform(X[i])
    
    # Convert to PyTorch tensor
    X_tensor = torch.FloatTensor(X_scaled)
    
    # Make predictions
    model.eval()
    with torch.no_grad():
        y_pred = model(X_tensor).numpy()
    
    return y_pred.flatten()

# Function to make ensemble predictions
def make_ensemble_predictions(models, data, scaler, model_params, feature_columns, weights, target_idx):
    # Create sequences
    X = create_sequences(data, model_params['seq_length'], feature_columns)
    
    # Scale the data
    X_scaled = np.zeros_like(X)
    for i in range(len(X)):
        X_scaled[i] = scaler.transform(X[i])
    
    # Convert to PyTorch tensor
    X_tensor = torch.FloatTensor(X_scaled)
    
    # Make predictions from each model
    preds = []
    for i, model_name in enumerate(['LSTM', 'GRU', 'RNN']):
        with torch.no_grad():
            y_pred = models[model_name](X_tensor).numpy()
        preds.append(y_pred)
    
    # Combine predictions using weighted average
    ensemble_pred = np.zeros_like(preds[0])
    for i, pred in enumerate(preds):
        ensemble_pred += weights[i] * pred
    
    return ensemble_pred.flatten()

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    return {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2
    }

# Function to make future predictions
def make_future_prediction(model, data, scaler, model_params, feature_columns, target_idx):
    # Get the last sequence from the data
    last_sequence = data[feature_columns].values[-model_params['seq_length']:]
    
    # Scale the sequence
    last_sequence_scaled = scaler.transform(last_sequence)
    
    # Convert to PyTorch tensor
    X_tensor = torch.FloatTensor(last_sequence_scaled).unsqueeze(0)
    
    # Make prediction for the next day
    model.eval()
    with torch.no_grad():
        next_day_pred = model(X_tensor).numpy()
    
    return next_day_pred[0, 0]

# Function to make ensemble future prediction
def make_ensemble_future_prediction(models, data, scaler, model_params, feature_columns, weights, target_idx):
    # Get predictions from each model
    future_preds = []
    for model_name in ['LSTM', 'GRU', 'RNN']:
        future_preds.append(
            make_future_prediction(
                models[model_name],
                data,
                scaler,
                model_params,
                feature_columns,
                target_idx
            )
        )
    
    # Combine predictions using weighted average
    future_pred = sum(w * p for w, p in zip(weights, future_preds))
    
    return future_pred

# Load models and data
try:
    models, model_params, data_info = load_models()
    
    if models is not None:
        # Get latest data
        with st.spinner("Fetching and processing latest data..."):
            df, sp500_raw, ibex35_raw = get_latest_data()
        
        # Display data info
        st.markdown(f'<div class="sub-header">{index_option} Data</div>', unsafe_allow_html=True)
        
        # Get ticker symbol and raw data based on selected index
        if index_option == "SP500":
            raw_data = sp500_raw
            target_idx = model_params.get('target_idx_sp500', -2)
            weights_key = 'ensemble_weights_sp500'
        else:
            raw_data = ibex35_raw
            target_idx = model_params.get('target_idx_ibex35', -1)
            weights_key = 'ensemble_weights_ibex35'
        
        # Display raw data chart
        st.write(f"Data range: {raw_data.index[0].date()} to {raw_data.index[-1].date()}")
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(raw_data.index, raw_data['Close'], label=f'{index_option} Close Price')
        ax.set_title(f'{index_option} Historical Close Price')
        ax.set_xlabel('Date')
        ax.set_ylabel('Price')
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        st.pyplot(fig)
        
        # Feature columns
        feature_columns = data_info['feature_columns']
        
        # Make predictions
        with st.spinner("Making predictions..."):
            if model_option != "Ensemble":
                predictions = make_predictions(
                    models[index_option][model_option],
                    df,
                    data_info['scaler'],
                    model_params,
                    feature_columns,
                    target_idx
                )
                
                # Make future prediction
                future_pred = make_future_prediction(
                    models[index_option][model_option],
                    df,
                    data_info['scaler'],
                    model_params,
                    feature_columns,
                    target_idx
                )
            else:
                # Get weights for ensemble
                weights = model_params.get(weights_key, [1/3, 1/3, 1/3])  # Default to equal weights
                
                predictions = make_ensemble_predictions(
                    models[index_option],
                    df,
                    data_info['scaler'],
                    model_params,
                    feature_columns,
                    weights,
                    target_idx
                )
                
                # Make ensemble future prediction
                future_pred = make_ensemble_future_prediction(
                    models[index_option],
                    df,
                    data_info['scaler'],
                    model_params,
                    feature_columns,
                    weights,
                    target_idx
                )
        
        # Get actual returns
        if index_option == "SP500":
            actual_returns = df['pct_change_sp500'].values
        else:
            actual_returns = df['pct_change_ibex'].values
        
        # Calculate metrics
        metrics = calculate_metrics(actual_returns[-len(predictions):], predictions)
        
        # Display metrics
        st.markdown(f'<div class="sub-header">Model Performance Metrics</div>', unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Mean Squared Error", f"{metrics['MSE']:.4f}")
        
        with col2:
            st.metric("Root Mean Squared Error", f"{metrics['RMSE']:.4f}")
        
        with col3:
            st.metric("Mean Absolute Error", f"{metrics['MAE']:.4f}")
        
        with col4:
            st.metric("RÂ² Score", f"{metrics['R2']:.4f}")
        
        # Plot the results
        st.markdown(f'<div class="sub-header">{model_option} Model Predictions for {index_option}</div>', unsafe_allow_html=True)
        
        # Create a DataFrame with actual and predicted values
        results_df = pd.DataFrame({
            'Date': df.index[-len(predictions):],
            'Actual': actual_returns[-len(predictions):],
            'Predicted': predictions
        })
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(results_df['Date'], results_df['Actual'], label='Actual Returns', color='blue')
        ax.plot(results_df['Date'], results_df['Predicted'], label='Predicted Returns', color='red')
        
        # Add a point for the future prediction
        next_date = results_df['Date'].iloc[-1] + pd.Timedelta(days=1)
        ax.scatter(next_date, future_pred, color='green', s=100, label='Next Day Prediction')
        
        ax.set_title(f'{model_option} Model Predictions for {index_option} Returns')
        ax.set_xlabel('Date')
        ax.set_ylabel('Returns (%)')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Format x-axis to show fewer dates
        plt.xticks(rotation=45)
        
        # Show the plot
        st.pyplot(fig)
        
        # Display the latest prediction and future prediction
        st.markdown('<div class="sub-header">Latest and Next Day Predictions</div>', unsafe_allow_html=True)
        
        latest_date = results_df['Date'].iloc[-1]
        latest_actual = results_df['Actual'].iloc[-1]
        latest_pred = results_df['Predicted'].iloc[-1]
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                f"Latest Actual ({latest_date.date()})",
                f"{latest_actual:.4f}%"
            )
        
        with col2:
            st.metric(
                "Latest Predicted",
                f"{latest_pred:.4f}%",
                delta=f"{latest_pred - latest_actual:.4f}%"
            )
        
        with col3:
            error_pct = abs(latest_actual - latest_pred) / abs(latest_actual) * 100 if latest_actual != 0 else 0
            st.metric(
                "Error Percentage",
                f"{error_pct:.2f}%"
            )
        
        with col4:
            st.metric(
                f"Next Day Prediction ({next_date.date()})",
                f"{future_pred:.4f}%",
                delta=f"{future_pred - latest_actual:.4f}%"
            )
        
        # Feature importance visualization
        st.markdown('<div class="sub-header">Feature Importance Analysis</div>', unsafe_allow_html=True)
        
        # For demonstration purposes, we'll create a simple feature importance visualization
        # In a real app, you would calculate feature importance using techniques like SHAP values
        
        # Select top 15 features for visualization
        top_features = 15
        feature_importance = np.random.rand(len(feature_columns))
        feature_indices = np.argsort(feature_importance)[-top_features:]
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.barh([feature_columns[i] for i in feature_indices], 
                [feature_importance[i] for i in feature_indices])
        ax.set_title(f'Top {top_features} Feature Importance for {index_option} Prediction')
        ax.set_xlabel('Importance')
        plt.tight_layout()
        st.pyplot(fig)
        
        # Show raw data in an expandable section
        with st.expander("Show Raw Data"):
            st.dataframe(results_df)
    else:
        st.warning("Please run the Jupyter notebook first to train and save the models.")
except Exception as e:
    st.error(f"An error occurred: {str(e)}")
    st.info("This is a demonstration version. In a full deployment, the app would connect to trained models.")
