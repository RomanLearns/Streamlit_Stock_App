{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Index Prediction using Deep Learning Models\n",
    "\n",
    "This notebook implements LSTM, GRU, and RNN models to predict the future prices of SP500 and IBEX35 indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "We'll load the data from the CSV files created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Check if CSV files exist, otherwise download data\n",
    "if os.path.exists('sp500_data.csv') and os.path.exists('ibex35_data.csv'):\n",
    "    sp500_data = pd.read_csv('sp500_data.csv', index_col='Date', parse_dates=True)\n",
    "    ibex35_data = pd.read_csv('ibex35_data.csv', index_col='Date', parse_dates=True)\n",
    "    print(\"Data loaded from CSV files.\")\n",
    "else:\n",
    "    # Define the symbols of the indices\n",
    "\n",
    "    sp500_ticker = \"^GSPC\"  # S&P 500\n",
    "    ibex35_ticker = \"^IBEX\"  # IBEX 35\n",
    "    \n",
    "    # Define the date range\n",
    "    start_date = \"2000-01-01\"\n",
    "    end_date = \"2024-01-01\"\n",
    "    \n",
    "    # Download historical data from Yahoo Finance\n",
    "    #sp500_data = yf.download(sp500_ticker, start=start_date, end=end_date)\n",
    "    sp500_data = yf.Tickers(sp500_ticker, ibex35_ticker).history(period=\"1d\", start=start_date, end=end_date)[\"Close\"]\n",
    "\n",
    "    #ibex35_data = yf.download(ibex35_ticker, start=start_date, end=end_date)\n",
    "    ibex35_data = yf.Ticker(ibex35_ticker).history(period=\"1d\", start=start_date, end=end_date)[\"Close\"]\n",
    "\n",
    "    \n",
    "    # Save the data to CSV\n",
    "    sp500_data.to_csv(\"sp500_data.csv\")\n",
    "    ibex35_data.to_csv(\"ibex35_data.csv\")\n",
    "    print(\"Data downloaded and saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 Data:\n",
      "Empty DataFrame\n",
      "Columns: [Close]\n",
      "Index: []\n",
      "\n",
      "IBEX 35 Data:\n",
      "Empty DataFrame\n",
      "Columns: [Close]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of each dataset\n",
    "print(\"S&P 500 Data:\")\n",
    "print(sp500_data.head())\n",
    "print(\"\\nIBEX 35 Data:\")\n",
    "print(ibex35_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Deep Learning Models\n",
    "\n",
    "For deep learning models, we need to prepare the data in a specific format. We'll create sequences of data for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for time series forecasting\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences of data for time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): Input data array\n",
    "    seq_length (int): Length of each sequence\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) where X is the input sequences and y is the target values\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length, 0])  # Predict the Close price\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for SP500\n",
    "def prepare_data(data, seq_length=60, test_size=0.2, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Prepare data for deep learning models.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): Input data\n",
    "    seq_length (int): Length of each sequence\n",
    "    test_size (float): Proportion of data to use for testing\n",
    "    feature_columns (list): List of column names to use as features\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, X_test, y_train, y_test, scaler)\n",
    "    \"\"\"\n",
    "    # Select features\n",
    "    if feature_columns is None:\n",
    "        # Use only OHLC and Volume as features by default\n",
    "        feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    data_features = data[feature_columns].values\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data_features)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(data_scaled, seq_length)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_train = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "    y_test = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for SP500 and IBEX35\n",
    "seq_length = 60  # Use 60 days of data to predict the next day\n",
    "\n",
    "# For SP500\n",
    "X_train_sp500, X_test_sp500, y_train_sp500, y_test_sp500, scaler_sp500 = prepare_data(\n",
    "    sp500_data, seq_length=seq_length\n",
    ")\n",
    "\n",
    "# For IBEX35\n",
    "X_train_ibex35, X_test_ibex35, y_train_ibex35, y_test_ibex35, scaler_ibex35 = prepare_data(\n",
    "    ibex35_data, seq_length=seq_length\n",
    ")\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(f\"SP500 - X_train: {X_train_sp500.shape}, X_test: {X_test_sp500.shape}, y_train: {y_train_sp500.shape}, y_test: {y_test_sp500.shape}\")\n",
    "print(f\"IBEX35 - X_train: {X_train_ibex35.shape}, X_test: {X_test_ibex35.shape}, y_train: {y_train_ibex35.shape}, y_test: {y_test_ibex35.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "def create_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create DataLoader for batch processing.\n",
    "    \n",
    "    Parameters:\n",
    "    X (torch.Tensor): Input data\n",
    "    y (torch.Tensor): Target data\n",
    "    batch_size (int): Batch size\n",
    "    shuffle (bool): Whether to shuffle the data\n",
    "    \n",
    "    Returns:\n",
    "    torch.utils.data.DataLoader: DataLoader object\n",
    "    \"\"\"\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_sp500 = create_dataloader(X_train_sp500, y_train_sp500, batch_size=batch_size)\n",
    "test_loader_sp500 = create_dataloader(X_test_sp500, y_test_sp500, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_ibex35 = create_dataloader(X_train_ibex35, y_train_ibex35, batch_size=batch_size)\n",
    "test_loader_ibex35 = create_dataloader(X_test_ibex35, y_test_ibex35, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM Model Implementation\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        LSTM model for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of features in the input\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        output_size (int): Number of features in the output\n",
    "        dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100, early_stopping_patience=10):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Model to train\n",
    "    train_loader (torch.utils.data.DataLoader): Training data loader\n",
    "    test_loader (torch.utils.data.DataLoader): Testing data loader\n",
    "    criterion (torch.nn.Module): Loss function\n",
    "    optimizer (torch.optim.Optimizer): Optimizer\n",
    "    num_epochs (int): Number of epochs to train for\n",
    "    early_stopping_patience (int): Number of epochs to wait for improvement before stopping\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, train_losses, test_losses)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                \n",
    "                test_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader, scaler, feature_columns):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Model to evaluate\n",
    "    test_loader (torch.utils.data.DataLoader): Testing data loader\n",
    "    scaler (sklearn.preprocessing.MinMaxScaler): Scaler used to normalize the data\n",
    "    feature_columns (list): List of column names used as features\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (y_true, y_pred, metrics)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            batch_pred = model(X_batch)\n",
    "            \n",
    "            # Move to CPU for numpy conversion\n",
    "            y_batch = y_batch.cpu().numpy()\n",
    "            batch_pred = batch_pred.cpu().numpy()\n",
    "            \n",
    "            # Inverse transform to get actual prices\n",
    "            # Create dummy arrays with the same shape as the original data\n",
    "            dummy = np.zeros((len(y_batch), len(feature_columns)))\n",
    "            dummy[:, 3] = y_batch.flatten()  # Assuming Close is at index 3\n",
    "            y_true_inv = scaler.inverse_transform(dummy)[:, 3]\n",
    "            \n",
    "            dummy[:, 3] = batch_pred.flatten()\n",
    "            y_pred_inv = scaler.inverse_transform(dummy)[:, 3]\n",
    "            \n",
    "            y_true.extend(y_true_inv)\n",
    "            y_pred.extend(y_pred_inv)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    return y_true, y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the results\n",
    "def plot_results(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot the true vs predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True values\n",
    "    y_pred (numpy.ndarray): Predicted values\n",
    "    title (str): Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label='True')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model for SP500\n",
    "input_size = X_train_sp500.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "\n",
    "lstm_model_sp500 = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model_sp500.parameters(), lr=0.001)\n",
    "\n",
    "lstm_model_sp500, train_losses_sp500, test_losses_sp500 = train_model(\n",
    "    lstm_model_sp500, train_loader_sp500, test_loader_sp500, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model for SP500\n",
    "feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "y_true_sp500, y_pred_sp500, metrics_sp500 = evaluate_model(\n",
    "    lstm_model_sp500, test_loader_sp500, scaler_sp500, feature_columns\n",
    ")\n",
    "\n",
    "print(\"LSTM Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500, y_pred_sp500, 'LSTM Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model for IBEX35\n",
    "input_size = X_train_ibex35.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "\n",
    "lstm_model_ibex35 = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model_ibex35.parameters(), lr=0.001)\n",
    "\n",
    "lstm_model_ibex35, train_losses_ibex35, test_losses_ibex35 = train_model(\n",
    "    lstm_model_ibex35, train_loader_ibex35, test_loader_ibex35, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model for IBEX35\n",
    "y_true_ibex35, y_pred_ibex35, metrics_ibex35 = evaluate_model(\n",
    "    lstm_model_ibex35, test_loader_ibex35, scaler_ibex35, feature_columns\n",
    ")\n",
    "\n",
    "print(\"LSTM Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35, y_pred_ibex35, 'LSTM Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRU Model Implementation\n",
    "\n",
    "Gated Recurrent Unit (GRU) is a type of recurrent neural network that is similar to LSTM but has fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        GRU model for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of features in the input\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        output_size (int): Number of features in the output\n",
    "        dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model for SP500\n",
    "input_size = X_train_sp500.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "\n",
    "gru_model_sp500 = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model_sp500.parameters(), lr=0.001)\n",
    "\n",
    "gru_model_sp500, train_losses_sp500_gru, test_losses_sp500_gru = train_model(\n",
    "    gru_model_sp500, train_loader_sp500, test_loader_sp500, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU model for SP500\n",
    "y_true_sp500_gru, y_pred_sp500_gru, metrics_sp500_gru = evaluate_model(\n",
    "    gru_model_sp500, test_loader_sp500, scaler_sp500, feature_columns\n",
    ")\n",
    "\n",
    "print(\"GRU Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500_gru.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500_gru, y_pred_sp500_gru, 'GRU Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model for IBEX35\n",
    "input_size = X_train_ibex35.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "\n",
    "gru_model_ibex35 = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model_ibex35.parameters(), lr=0.001)\n",
    "\n",
    "gru_model_ibex35, train_losses_ibex35_gru, test_losses_ibex35_gru = train_model(\n",
    "    gru_model_ibex35, train_loader_ibex35, test_loader_ibex35, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU model for IBEX35\n",
    "y_true_ibex35_gru, y_pred_ibex35_gru, metrics_ibex35_gru = evaluate_model(\n",
    "    gru_model_ibex35, test_loader_ibex35, scaler_ibex35, feature_columns\n",
    ")\n",
    "\n",
    "print(\"GRU Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35_gru.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35_gru, y_pred_ibex35_gru, 'GRU Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNN Model Implementation\n",
    "\n",
    "Simple Recurrent Neural Network (RNN) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        RNN model for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of features in the input\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        output_size (int): Number of features in the output\n",
    "        dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN model for SP500\n",
    "input_size = X_train_sp500.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "\n",
    "rnn_model_sp500 = RNNModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model_sp500.parameters(), lr=0.001)\n",
    "\n",
    "rnn_model_sp500, train_losses_sp500_rnn, test_losses_sp500_rnn = train_model(\n",
    "    rnn_model_sp500, train_loader_sp500, test_loader_sp500, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RNN model for SP500\n",
    "y_true_sp500_rnn, y_pred_sp500_rnn, metrics_sp500_rnn = evaluate_model(\n",
    "    rnn_model_sp500, test_loader_sp500, scaler_sp500, feature_columns\n",
    ")\n",
    "\n",
    "print(\"RNN Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500_rnn.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500_rnn, y_pred_sp500_rnn, 'RNN Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN model for IBEX35\n",
    "input_size = X_train_ibex35.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "\n",
    "rnn_model_ibex35 = RNNModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model_ibex35.parameters(), lr=0.001)\n",
    "\n",
    "rnn_model_ibex35, train_losses_ibex35_rnn, test_losses_ibex35_rnn = train_model(\n",
    "    rnn_model_ibex35, train_loader_ibex35, test_loader_ibex35, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RNN model for IBEX35\n",
    "y_true_ibex35_rnn, y_pred_ibex35_rnn, metrics_ibex35_rnn = evaluate_model(\n",
    "    rnn_model_ibex35, test_loader_ibex35, scaler_ibex35, feature_columns\n",
    ")\n",
    "\n",
    "print(\"RNN Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35_rnn.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35_rnn, y_pred_ibex35_rnn, 'RNN Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Model Implementation\n",
    "\n",
    "Create an ensemble model by combining the predictions from LSTM, GRU, and RNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create ensemble predictions\n",
    "def ensemble_predictions(models, test_loader, scaler, feature_columns, weights=None):\n",
    "    \"\"\"\n",
    "    Create ensemble predictions by combining the predictions from multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    models (list): List of models\n",
    "    test_loader (torch.utils.data.DataLoader): Testing data loader\n",
    "    scaler (sklearn.preprocessing.MinMaxScaler): Scaler used to normalize the data\n",
    "    feature_columns (list): List of column names used as features\n",
    "    weights (list): List of weights for each model (default: equal weights)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (y_true, y_pred, metrics)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "    # Set equal weights if not provided\n",
    "    if weights is None:\n",
    "        weights = [1/len(models)] * len(models)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Get predictions from each model\n",
    "            batch_preds = []\n",
    "            for model in models:\n",
    "                batch_pred = model(X_batch).cpu().numpy()\n",
    "                batch_preds.append(batch_pred)\n",
    "            \n",
    "            # Combine predictions using weighted average\n",
    "            ensemble_pred = np.zeros_like(batch_preds[0])\n",
    "            for i, pred in enumerate(batch_preds):\n",
    "                ensemble_pred += weights[i] * pred\n",
    "            \n",
    "            # Move to CPU for numpy conversion\n",
    "            y_batch = y_batch.cpu().numpy()\n",
    "            \n",
    "            # Inverse transform to get actual prices\n",
    "            # Create dummy arrays with the same shape as the original data\n",
    "            dummy = np.zeros((len(y_batch), len(feature_columns)))\n",
    "            dummy[:, 3] = y_batch.flatten()  # Assuming Close is at index 3\n",
    "            y_true_inv = scaler.inverse_transform(dummy)[:, 3]\n",
    "            \n",
    "            dummy[:, 3] = ensemble_pred.flatten()\n",
    "            y_pred_inv = scaler.inverse_transform(dummy)[:, 3]\n",
    "            \n",
    "            y_true.extend(y_true_inv)\n",
    "            y_pred.extend(y_pred_inv)\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    return y_true, y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions for SP500\n",
    "models_sp500 = [lstm_model_sp500, gru_model_sp500, rnn_model_sp500]\n",
    "\n",
    "# Use weights based on individual model performance (inverse of MSE)\n",
    "mse_values = [metrics_sp500['MSE'], metrics_sp500_gru['MSE'], metrics_sp500_rnn['MSE']]\n",
    "weights = [1/mse for mse in mse_values]\n",
    "weights = [w/sum(weights) for w in weights]  # Normalize weights\n",
    "\n",
    "print(f\"Ensemble weights for SP500: {weights}\")\n",
    "\n",
    "y_true_sp500_ensemble, y_pred_sp500_ensemble, metrics_sp500_ensemble = ensemble_predictions(\n",
    "    models_sp500, test_loader_sp500, scaler_sp500, feature_columns, weights=weights\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500_ensemble.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500_ensemble, y_pred_sp500_ensemble, 'Ensemble Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions for IBEX35\n",
    "models_ibex35 = [lstm_model_ibex35, gru_model_ibex35, rnn_model_ibex35]\n",
    "\n",
    "# Use weights based on individual model performance (inverse of MSE)\n",
    "mse_values = [metrics_ibex35['MSE'], metrics_ibex35_gru['MSE'], metrics_ibex35_rnn['MSE']]\n",
    "weights = [1/mse for mse in mse_values]\n",
    "weights = [w/sum(weights) for w in weights]  # Normalize weights\n",
    "\n",
    "print(f\"Ensemble weights for IBEX35: {weights}\")\n",
    "\n",
    "y_true_ibex35_ensemble, y_pred_ibex35_ensemble, metrics_ibex35_ensemble = ensemble_predictions(\n",
    "    models_ibex35, test_loader_ibex35, scaler_ibex35, feature_columns, weights=weights\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35_ensemble.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35_ensemble, y_pred_ibex35_ensemble, 'Ensemble Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance for SP500\n",
    "models_sp500 = ['LSTM', 'GRU', 'RNN', 'Ensemble']\n",
    "metrics_list_sp500 = [metrics_sp500, metrics_sp500_gru, metrics_sp500_rnn, metrics_sp500_ensemble]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_sp500 = pd.DataFrame(index=models_sp500)\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R2']:\n",
    "    comparison_sp500[metric] = [metrics[metric] for metrics in metrics_list_sp500]\n",
    "\n",
    "print(\"Model Performance Comparison for SP500:\")\n",
    "print(comparison_sp500)\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['MSE', 'RMSE', 'MAE']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    plt.bar(models_sp500, comparison_sp500[metric])\n",
    "    plt.title(f'{metric} Comparison for SP500')\n",
    "    plt.ylabel(metric)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 separately (higher is better)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(models_sp500, comparison_sp500['R2'])\n",
    "plt.title('R2 Comparison for SP500')\n",
    "plt.ylabel('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance for IBEX35\n",
    "models_ibex35 = ['LSTM', 'GRU', 'RNN', 'Ensemble']\n",
    "metrics_list_ibex35 = [metrics_ibex35, metrics_ibex35_gru, metrics_ibex35_rnn, metrics_ibex35_ensemble]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_ibex35 = pd.DataFrame(index=models_ibex35)\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R2']:\n",
    "    comparison_ibex35[metric] = [metrics[metric] for metrics in metrics_list_ibex35]\n",
    "\n",
    "print(\"Model Performance Comparison for IBEX35:\")\n",
    "print(comparison_ibex35)\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['MSE', 'RMSE', 'MAE']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    plt.bar(models_ibex35, comparison_ibex35[metric])\n",
    "    plt.title(f'{metric} Comparison for IBEX35')\n",
    "    plt.ylabel(metric)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 separately (higher is better)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(models_ibex35, comparison_ibex35['R2'])\n",
    "plt.title('R2 Comparison for IBEX35')\n",
    "plt.ylabel('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models for Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for models if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Function to save a model\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Model to save\n",
    "    filename (str): Filename to save the model to\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# Save SP500 models\n",
    "save_model(lstm_model_sp500, 'models/lstm_sp500.pth')\n",
    "save_model(gru_model_sp500, 'models/gru_sp500.pth')\n",
    "save_model(rnn_model_sp500, 'models/rnn_sp500.pth')\n",
    "\n",
    "# Save IBEX35 models\n",
    "save_model(lstm_model_ibex35, 'models/lstm_ibex35.pth')\n",
    "save_model(gru_model_ibex35, 'models/gru_ibex35.pth')\n",
    "save_model(rnn_model_ibex35, 'models/rnn_ibex35.pth')\n",
    "\n",
    "# Save model parameters for later use\n",
    "model_params = {\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers,\n",
    "    'output_size': output_size,\n",
    "    'dropout': dropout,\n",
    "    'seq_length': seq_length,\n",
    "    'feature_columns': feature_columns,\n",
    "    'ensemble_weights_sp500': weights,\n",
    "    'ensemble_weights_ibex35': weights\n",
    "}\n",
    "\n",
    "with open('models/model_params.pkl', 'wb') as f:\n",
    "    pickle.dump(model_params, f)\n",
    "print(\"Model parameters saved to models/model_params.pkl\")\n",
    "\n",
    "# Save scalers\n",
    "with open('models/scaler_sp500.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_sp500, f)\n",
    "print(\"SP500 scaler saved to models/scaler_sp500.pkl\")\n",
    "\n",
    "with open('models/scaler_ibex35.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_ibex35, f)\n",
    "print(\"IBEX35 scaler saved to models/scaler_ibex35.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we have implemented LSTM, GRU, and RNN models to predict the future prices of SP500 and IBEX35 indices. We have also created an ensemble model by combining the predictions from these models. The models have been saved for later use in the Streamlit app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
