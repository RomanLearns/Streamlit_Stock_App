{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Stock Index Prediction using Deep Learning Models\n",
    "\n",
    "This notebook implements LSTM, GRU, and RNN models to predict the future prices of SP500 and IBEX35 indices using enhanced features including economic indicators, technical analysis, and seasonality decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import pandas_ta as ta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "from arch import arch_model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "We'll download data for SP500 and IBEX35 indices from Yahoo Finance and economic data from FRED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "^GSPC: No price data found, symbol may be delisted (1d 2000-01-01 -> 2024-01-01)\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['^IBEX']: JSONDecodeError('Expecting value: line 1 column 1 (char 0)')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded and saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Define the symbols of the indices\n",
    "sp500_ticker = \"^GSPC\"  # S&P 500\n",
    "ibex35_ticker = \"^IBEX\"  # IBEX 35\n",
    "\n",
    "# Define the date range\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-01-01\"\n",
    "\n",
    "# Download historical data from Yahoo Finance\n",
    "if not os.path.exists(\"sp500_data.csv\") or not os.path.exists(\"ibex35_data.csv\"):\n",
    "    print(\"Downloading data from Yahoo Finance...\")\n",
    "    sp500_data = yf.download(sp500_ticker).history(period='1d', start=start_date, end=end_date)\n",
    "    ibex35_data = yf.download(ibex35_ticker, start=start_date, end=end_date)\n",
    "\n",
    "    # Save the data to CSV\n",
    "    sp500_data.to_csv(\"sp500_data.csv\")\n",
    "    ibex35_data.to_csv(\"ibex35_data.csv\")\n",
    "\n",
    "    print(\"Data downloaded and saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 Data:\n",
      "Empty DataFrame\n",
      "Columns: [Open, High, Low, Close, Adj Close, Volume]\n",
      "Index: []\n",
      "\n",
      "IBEX 35 Data:\n",
      "Empty DataFrame\n",
      "Columns: [Open, High, Low, Close, Adj Close, Volume]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of each dataset\n",
    "print(\"S&P 500 Data:\")\n",
    "print(sp500_data.head())\n",
    "print(\"\\nIBEX 35 Data:\")\n",
    "print(ibex35_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collecting Economic Data from FRED\n",
    "\n",
    "We'll collect economic indicators from the Federal Reserve Economic Data (FRED) to enhance our prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRED API Key\n",
    "FRED_API_KEY = \"144e929e4cb64cc8c55902991441556b\"\n",
    "\n",
    "# Economic factors to collect\n",
    "economic_factors = [\"DFF\", \"T10YIE\", \"DCOILWTICO\", \"VIXCLS\", \"DEXUSEU\", \"NASDAQCOM\", \"DHHNGSP\", \n",
    "                    \"BAMLC0A0CMEY\", \"USEPUINDXD\", \"DPRIME\", \"DEXCAUS\", \"DEXDNUS\", \"DEXJPUS\", \"DEXCHUS\", \"DEXSZUS\",\n",
    "                    \"DEXUSAL\", \"DEXUSUK\", \"DEXKOUS\", \"INFECTDISEMVTRACKD\", \"DCOILBRENTEU\", \"DEXMXUS\"]\n",
    "\n",
    "# Rename mapping for better readability\n",
    "rename_map = {\"DFF\":\"daily_fed_funds\", \"T10YIE\":\"ten_year_breakeven_inflation\", \"DCOILWTICO\":\"WTI_crude_oil_price\", \n",
    "              \"VIXCLS\":\"VIX\", \"DEXUSEU\":\"USD$EUR_spot\", \"NASDAQCOM\":\"NASDAQ\", \"DHHNGSP\":\"henry_hub_natrual_gas\",\n",
    "              \"BAMLC0A0CMEY\":\"corporate_bond_yield\", \"USEPUINDXD\":\"economic_policy_uncertainty\", \"DPRIME\":\"prime_rate\",\n",
    "              \"DEXCAUS\":\"USD$CAD_spot\", \"DEXDNUS\":\"USD$CNY_spot\", \"DEXJPUS\":\"USD$JPY_spot\", \"DEXCHUS\":\"USD$CHF_spot\", \n",
    "              \"DEXSZUS\":\"USD$SEK_spot\", \"DEXUSAL\":\"USD$AUD_spot\", \"DEXUSUK\":\"USD$GBP_spot\", \"DEXKOUS\":\"USD$KRW_spot\", \n",
    "              \"INFECTDISEMVTRACKD\":\"infectious_disease\", \"DCOILBRENTEU\":\"Brent_crude_oil_price_europe\", \"DEXMXUS\":\"USD$MXN_spot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if economic data already exists\n",
    "if os.path.exists('economic_data.csv'):\n",
    "    print(\"Loading economic data from CSV file...\")\n",
    "    df_total = pd.read_csv('economic_data.csv')\n",
    "    df_total['date'] = pd.to_datetime(df_total['date'])\n",
    "else:\n",
    "    print(\"Downloading economic data from FRED...\")\n",
    "    # Pull the FRED data for daily economic variables\n",
    "    df_total = pd.DataFrame()\n",
    "    \n",
    "    for series in economic_factors:\n",
    "        curr_start, curr_end = start_date, \"2005-01-01\"\n",
    "        series_df = pd.DataFrame()\n",
    "        # getting a given series\n",
    "        while curr_start <= end_date:\n",
    "            base_series_url = f\"https://api.stlouisfed.org/fred/series/observations?series_id={series}&api_key={FRED_API_KEY}&file_type=json&observation_start={curr_start}&observation_end={curr_end}&units=lin&frequency=d\"\n",
    "            response = requests.get(base_series_url)\n",
    "            data = response.json()\n",
    "            df = pd.json_normalize(data)\n",
    "            observations = pd.json_normalize(df['observations'][0])\n",
    "            series_df = pd.concat([series_df, observations], axis=0)\n",
    "            curr_start = (pd.to_datetime(curr_end) + relativedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            curr_end = min((pd.to_datetime(curr_end) + relativedelta(years=5, days=1), pd.to_datetime(end_date))).strftime(\"%Y-%m-%d\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # renaming columns\n",
    "        series_df.rename(columns={\"date\":\"date\", \"value\":rename_map[series]}, inplace=True)\n",
    "        series_df.drop(columns=[\"realtime_start\", \"realtime_end\"], inplace=True)\n",
    "        \n",
    "        if df_total.empty:\n",
    "            df_total = series_df\n",
    "        else:\n",
    "            df_total = pd.merge(df_total, series_df, on=['date'], how='inner', validate='1:1')\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_total.to_csv('economic_data.csv', index=False)\n",
    "    print(\"Economic data downloaded and saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of economic data\n",
    "print(\"Economic Data:\")\n",
    "print(df_total.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Technical Analysis\n",
    "\n",
    "We'll add technical indicators and perform feature engineering to enhance our prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess IBEX 35 data\n",
    "ibex = pd.read_csv('ibex35_data.csv')\n",
    "ibex = ta.add_all_ta_features(ibex, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "ibex.index = pd.to_datetime(ibex['Date'])\n",
    "ibex.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "# Load and preprocess S&P 500 data\n",
    "sp500 = pd.read_csv('sp500_data.csv')\n",
    "sp500 = ta.add_all_ta_features(sp500, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "sp500.index = pd.to_datetime(sp500['Date'])\n",
    "sp500.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage changes and add month, day, year columns\n",
    "# IBEX 35\n",
    "ibex['pct_change'] = ibex['Close'].pct_change() * 100  # Percentage change in Close\n",
    "ibex['month'] = ibex.index.month\n",
    "ibex['day'] = ibex.index.day\n",
    "ibex['year'] = ibex.index.year\n",
    "\n",
    "# S&P 500\n",
    "sp500['pct_change'] = sp500['Close'].pct_change() * 100  # Percentage change in Close\n",
    "sp500['month'] = sp500.index.month\n",
    "sp500['day'] = sp500.index.day\n",
    "sp500['year'] = sp500.index.year\n",
    "\n",
    "# Drop NaNs from percentage changes\n",
    "ibex = ibex.dropna()\n",
    "sp500 = sp500.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Seasonality Decomposition\n",
    "\n",
    "We'll decompose the time series to remove seasonality and improve our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shift values to handle negative values for multiplicative decomposition of pct_change\n",
    "shift_value_ibex = abs(ibex['pct_change'].min()) + 1\n",
    "ibex['pct_change_shifted'] = ibex['pct_change'] + shift_value_ibex\n",
    "\n",
    "shift_value_sp500 = abs(sp500['pct_change'].min()) + 1\n",
    "sp500['pct_change_shifted'] = sp500['pct_change'] + shift_value_sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decompose and remove multiplicative seasonality using seasonal_decompose (Classic)\n",
    "def decompose_and_remove_seasonality_classic(series, title, period, freq_label, shift_value):\n",
    "    # Decompose the series using a multiplicative model\n",
    "    decomposition = seasonal_decompose(series, model='multiplicative', period=period)\n",
    "    \n",
    "    # Remove the seasonal component\n",
    "    deseasonalized = series / decomposition.seasonal\n",
    "    \n",
    "    # Shift back to original scale\n",
    "    deseasonalized_unshifted = deseasonalized - shift_value\n",
    "    \n",
    "    # Plot the original series, trend, seasonal component, and deseasonalized series\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(411)\n",
    "    plt.plot(series - shift_value, label='Original')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - Classic Multiplicative Decomposition (Period: {freq_label})')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend - shift_value, label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal, label='Seasonal Component')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(deseasonalized_unshifted, label='Deseasonalized')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return decomposition, deseasonalized_unshifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decompose and remove seasonality using STL (modified to return trend + residual)\n",
    "def decompose_and_remove_seasonality_stl(series, title, period, freq_label, shift_value=None, model='multiplicative'):\n",
    "    # Decompose the series using STL\n",
    "    stl = STL(series, period=period, seasonal=13)  # seasonal=13 ensures a smooth seasonal component\n",
    "    decomposition = stl.fit()\n",
    "    \n",
    "    # Remove the seasonal component\n",
    "    if model == 'multiplicative':\n",
    "        # For multiplicative model: deseasonalized = trend * residual\n",
    "        deseasonalized = series / decomposition.seasonal\n",
    "    else:\n",
    "        # For additive model: deseasonalized = trend + residual\n",
    "        deseasonalized = series - decomposition.seasonal\n",
    "    \n",
    "    # If shift_value is provided, shift back to original scale\n",
    "    if shift_value is not None:\n",
    "        deseasonalized_unshifted = deseasonalized - shift_value\n",
    "        series_unshifted = series - shift_value\n",
    "    else:\n",
    "        deseasonalized_unshifted = deseasonalized\n",
    "        series_unshifted = series\n",
    "    \n",
    "    # Plot the original series, trend, seasonal component, and deseasonalized series\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(411)\n",
    "    plt.plot(series_unshifted, label='Original')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'{title} - STL Decomposition (Period: {freq_label})')\n",
    "    \n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend, label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal, label='Seasonal Component')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.subplot(414)\n",
    "    plt.plot(deseasonalized_unshifted, label='Deseasonalized (Trend + Residual)')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return decomposition, deseasonalized_unshifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply STL decomposition to Close prices\n",
    "# Perform STL decomposition on the Close prices (no shift needed since Close is positive)\n",
    "ibex_decomp_close_yearly_stl, ibex_close_deseasonalized_yearly_stl = decompose_and_remove_seasonality_stl(\n",
    "    ibex['Close'], 'IBEX 35 Close', period=252, freq_label='Yearly (252 days)', shift_value=None, model='additive'\n",
    ")\n",
    "# Use the deseasonalized series (trend + residual) as the seasonally adjusted Close\n",
    "ibex['Close_deseasonalized'] = ibex_close_deseasonalized_yearly_stl\n",
    "\n",
    "sp500_decomp_close_yearly_stl, sp500_close_deseasonalized_yearly_stl = decompose_and_remove_seasonality_stl(\n",
    "    sp500['Close'], 'S&P 500 Close', period=252, freq_label='Yearly (252 days)', shift_value=None, model='additive'\n",
    ")\n",
    "# Use the deseasonalized series (trend + residual) as the seasonally adjusted Close\n",
    "sp500['Close_deseasonalized'] = sp500_close_deseasonalized_yearly_stl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate percentage changes based on the deseasonalized Close (trend + residual)\n",
    "ibex['pct_change_deseasonalized'] = ibex['Close_deseasonalized'].pct_change() * 100\n",
    "sp500['pct_change_deseasonalized'] = sp500['Close_deseasonalized'].pct_change() * 100\n",
    "\n",
    "# Drop NaNs from the new percentage changes\n",
    "ibex = ibex.dropna()\n",
    "sp500 = sp500.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merging Data and Creating Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare economic data\n",
    "econ = df_total.copy()\n",
    "econ.rename(columns={'date':'Date'}, inplace=True)\n",
    "econ.index = pd.to_datetime(econ['Date'])\n",
    "econ.drop(columns=['Date'], inplace=True)\n",
    "for col in econ.select_dtypes(include='object'):\n",
    "    econ[col] = pd.to_numeric(econ[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SP500 and IBEX35 data\n",
    "df = pd.merge(sp500, ibex, how='inner', left_index=True, right_index=True, suffixes=[\"_sp500\", \"_ibex\"], validate='1:1')\n",
    "\n",
    "# Merge with economic data\n",
    "df = pd.merge(df, econ, how='inner', left_index=True, right_index=True, validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and rename columns\n",
    "pct_change_cols = [col for col in df.columns if 'pct_change' in col and 'deseasonalized' in col]\n",
    "df.drop(columns=['Close_sp500', 'Close_ibex'] + pct_change_cols, inplace=True)\n",
    "df.rename(columns={'Close_deseasonalized_sp500':'Close_sp500', 'Close_deseasonalized_ibex':'Close_ibex', 'pct_change_sp500':'SP500_Returns', 'pct_change_ibex':'IBEX35_Returns'}, inplace=True)\n",
    "\n",
    "# Deseasonalizing the returns\n",
    "df['SP500_Returns'] = df['Close_sp500'].pct_change() * 100\n",
    "df['IBEX35_Returns'] = df['Close_ibex'].pct_change() * 100\n",
    "\n",
    "# Create target variables (next day returns)\n",
    "df['SP500_Returns_Next'] = df['SP500_Returns'].shift(-1)\n",
    "df['IBEX35_Returns_Next'] = df['IBEX35_Returns'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features\n",
    "for lag in [1, 2, 3, 5, 7, 14]:\n",
    "    df[f'SP500_Returns_Lag{lag}'] = df['SP500_Returns'].shift(lag)\n",
    "    df[f'IBEX35_Returns_Lag{lag}'] = df['IBEX35_Returns'].shift(lag)\n",
    "\n",
    "# Create rolling statistics\n",
    "df['SP500_Rolling_Mean_5'] = df['SP500_Returns'].rolling(window=5).mean()\n",
    "df['SP500_Rolling_Vol_5'] = df['SP500_Returns'].rolling(window=5).std()\n",
    "df['IBEX35_Rolling_Mean_5'] = df['IBEX35_Returns'].rolling(window=5).mean()\n",
    "df['IBEX35_Rolling_Vol_5'] = df['IBEX35_Returns'].rolling(window=5).std()\n",
    "\n",
    "# Create lagged economic features\n",
    "econ_cols = econ.columns.tolist()\n",
    "for col in econ_cols:\n",
    "    df[f'{col}_Lag1'] = df[col].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add BIAS indicator\n",
    "df['SP500_BIAS'] = (df['Close_sp500'] - df['Close_sp500'].rolling(window=12).mean()) / df['Close_sp500'].rolling(window=12).mean() * 100\n",
    "df['IBEX35_BIAS'] = (df['Close_ibex'] - df['Close_ibex'].rolling(window=12).mean()) / df['Close_ibex'].rolling(window=12).mean() * 100\n",
    "\n",
    "# Add PSY (Psychological Line Indicator)\n",
    "def calculate_psy(price, period=12):\n",
    "    returns = price.pct_change()\n",
    "    psy = (returns > 0).rolling(window=period).sum() / period * 100\n",
    "    return psy\n",
    "\n",
    "df['SP500_PSY'] = calculate_psy(df['Close_sp500'])\n",
    "df['IBEX35_PSY'] = calculate_psy(df['Close_ibex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GARCH volatility\n",
    "try:\n",
    "    # Fit GARCH model for SP500\n",
    "    garch_model_sp500 = arch_model(df['SP500_Returns'].dropna(), vol='Garch', p=1, q=1)\n",
    "    garch_fit_sp500 = garch_model_sp500.fit(disp='off')\n",
    "    df['SP500_GARCH_Vol'] = pd.Series(garch_fit_sp500.conditional_volatility, index=garch_fit_sp500.conditional_volatility.index)\n",
    "    \n",
    "    # Fit GARCH model for IBEX35\n",
    "    garch_model_ibex35 = arch_model(df['IBEX35_Returns'].dropna(), vol='Garch', p=1, q=1)\n",
    "    garch_fit_ibex35 = garch_model_ibex35.fit(disp='off')\n",
    "    df['IBEX35_GARCH_Vol'] = pd.Series(garch_fit_ibex35.conditional_volatility, index=garch_fit_ibex35.conditional_volatility.index)\n",
    "except Exception as e:\n",
    "    print(f\"Error fitting GARCH model: {e}\")\n",
    "    # If GARCH fitting fails, use rolling volatility as a substitute\n",
    "    df['SP500_GARCH_Vol'] = df['SP500_Returns'].rolling(window=22).std()\n",
    "    df['IBEX35_GARCH_Vol'] = df['IBEX35_Returns'].rolling(window=22).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the dataframe\n",
    "df = df.dropna(axis=1, how='all').dropna(axis=0, how='any')\n",
    "print(f\"Final dataframe shape: {df.shape}\")\n",
    "print(f\"Number of features: {df.shape[1] - 2}\")  # Subtract 2 for the target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Preparation for Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for time series forecasting\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences of data for time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): Input data array\n",
    "    seq_length (int): Length of each sequence\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) where X is the input sequences and y is the target values\n",
    "    \"\"\"\n",
    "    X, y_sp500, y_ibex35 = [], [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :-2])  # All features except the target variables\n",
    "        y_sp500.append(data[i+seq_length, -2])  # SP500_Returns_Next\n",
    "        y_ibex35.append(data[i+seq_length, -1])  # IBEX35_Returns_Next\n",
    "    return np.array(X), np.array(y_sp500), np.array(y_ibex35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for deep learning models\n",
    "def prepare_data(data, seq_length=60, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for deep learning models.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): Input data\n",
    "    seq_length (int): Length of each sequence\n",
    "    test_size (float): Proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, X_test, y_train_sp500, y_test_sp500, y_train_ibex35, y_test_ibex35, scaler)\n",
    "    \"\"\"\n",
    "    # Convert to numpy array\n",
    "    data_values = data.values\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(data_values)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y_sp500, y_ibex35 = create_sequences(data_scaled, seq_length)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train_sp500, y_test_sp500 = y_sp500[:split_idx], y_sp500[split_idx:]\n",
    "    y_train_ibex35, y_test_ibex35 = y_ibex35[:split_idx], y_ibex35[split_idx:]\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_train_sp500 = torch.FloatTensor(y_train_sp500).reshape(-1, 1)\n",
    "    y_test_sp500 = torch.FloatTensor(y_test_sp500).reshape(-1, 1)\n",
    "    y_train_ibex35 = torch.FloatTensor(y_train_ibex35).reshape(-1, 1)\n",
    "    y_test_ibex35 = torch.FloatTensor(y_test_ibex35).reshape(-1, 1)\n",
    "    \n",
    "    return X_train, X_test, y_train_sp500, y_test_sp500, y_train_ibex35, y_test_ibex35, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for deep learning models\n",
    "seq_length = 30  # Use 30 days of data to predict the next day\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train_sp500, y_test_sp500, y_train_ibex35, y_test_ibex35, scaler = prepare_data(\n",
    "    df, seq_length=seq_length\n",
    ")\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train_sp500: {y_train_sp500.shape}, y_test_sp500: {y_test_sp500.shape}\")\n",
    "print(f\"y_train_ibex35: {y_train_ibex35.shape}, y_test_ibex35: {y_test_ibex35.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "def create_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create DataLoader for batch processing.\n",
    "    \n",
    "    Parameters:\n",
    "    X (torch.Tensor): Input data\n",
    "    y (torch.Tensor): Target data\n",
    "    batch_size (int): Batch size\n",
    "    shuffle (bool): Whether to shuffle the data\n",
    "    \n",
    "    Returns:\n",
    "    torch.utils.data.DataLoader: DataLoader object\n",
    "    \"\"\"\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_sp500 = create_dataloader(X_train, y_train_sp500, batch_size=batch_size)\n",
    "test_loader_sp500 = create_dataloader(X_test, y_test_sp500, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_ibex35 = create_dataloader(X_train, y_train_ibex35, batch_size=batch_size)\n",
    "test_loader_ibex35 = create_dataloader(X_test, y_test_ibex35, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LSTM Model Implementation\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        LSTM model for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of features in the input\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        output_size (int): Number of features in the output\n",
    "        dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100, early_stopping_patience=10):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Model to train\n",
    "    train_loader (torch.utils.data.DataLoader): Training data loader\n",
    "    test_loader (torch.utils.data.DataLoader): Testing data loader\n",
    "    criterion (torch.nn.Module): Loss function\n",
    "    optimizer (torch.optim.Optimizer): Optimizer\n",
    "    num_epochs (int): Number of epochs to train for\n",
    "    early_stopping_patience (int): Number of epochs to wait for improvement before stopping\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, train_losses, test_losses)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                \n",
    "                test_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader, scaler, feature_idx, target_idx):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Model to evaluate\n",
    "    test_loader (torch.utils.data.DataLoader): Testing data loader\n",
    "    scaler (sklearn.preprocessing.MinMaxScaler): Scaler used to normalize the data\n",
    "    feature_idx (int): Index of the feature in the original data\n",
    "    target_idx (int): Index of the target in the original data\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (y_true, y_pred, metrics)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            batch_pred = model(X_batch)\n",
    "            \n",
    "            # Move to CPU for numpy conversion\n",
    "            y_batch = y_batch.cpu().numpy()\n",
    "            batch_pred = batch_pred.cpu().numpy()\n",
    "            \n",
    "            y_true.extend(y_batch)\n",
    "            y_pred.extend(batch_pred)\n",
    "    \n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    return y_true, y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the results\n",
    "def plot_results(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot the true vs predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True values\n",
    "    y_pred (numpy.ndarray): Predicted values\n",
    "    title (str): Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label='True')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Returns (%)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model for SP500\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "dropout = 0.3\n",
    "\n",
    "lstm_model_sp500 = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model_sp500.parameters(), lr=0.001)\n",
    "\n",
    "lstm_model_sp500, train_losses_sp500, test_losses_sp500 = train_model(\n",
    "    lstm_model_sp500, train_loader_sp500, test_loader_sp500, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model for SP500\n",
    "y_true_sp500, y_pred_sp500, metrics_sp500 = evaluate_model(\n",
    "    lstm_model_sp500, test_loader_sp500, scaler, -2, -2\n",
    ")\n",
    "\n",
    "print(\"LSTM Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500, y_pred_sp500, 'LSTM Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model for IBEX35\n",
    "lstm_model_ibex35 = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lstm_model_ibex35.parameters(), lr=0.001)\n",
    "\n",
    "lstm_model_ibex35, train_losses_ibex35, test_losses_ibex35 = train_model(\n",
    "    lstm_model_ibex35, train_loader_ibex35, test_loader_ibex35, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model for IBEX35\n",
    "y_true_ibex35, y_pred_ibex35, metrics_ibex35 = evaluate_model(\n",
    "    lstm_model_ibex35, test_loader_ibex35, scaler, -1, -1\n",
    ")\n",
    "\n",
    "print(\"LSTM Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35, y_pred_ibex35, 'LSTM Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GRU Model Implementation\n",
    "\n",
    "Gated Recurrent Unit (GRU) is a type of recurrent neural network that is similar to LSTM but has fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        GRU model for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of features in the input\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        output_size (int): Number of features in the output\n",
    "        dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model for SP500\n",
    "gru_model_sp500 = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model_sp500.parameters(), lr=0.001)\n",
    "\n",
    "gru_model_sp500, train_losses_sp500_gru, test_losses_sp500_gru = train_model(\n",
    "    gru_model_sp500, train_loader_sp500, test_loader_sp500, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU model for SP500\n",
    "y_true_sp500_gru, y_pred_sp500_gru, metrics_sp500_gru = evaluate_model(\n",
    "    gru_model_sp500, test_loader_sp500, scaler, -2, -2\n",
    ")\n",
    "\n",
    "print(\"GRU Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500_gru.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500_gru, y_pred_sp500_gru, 'GRU Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRU model for IBEX35\n",
    "gru_model_ibex35 = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model_ibex35.parameters(), lr=0.001)\n",
    "\n",
    "gru_model_ibex35, train_losses_ibex35_gru, test_losses_ibex35_gru = train_model(\n",
    "    gru_model_ibex35, train_loader_ibex35, test_loader_ibex35, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRU model for IBEX35\n",
    "y_true_ibex35_gru, y_pred_ibex35_gru, metrics_ibex35_gru = evaluate_model(\n",
    "    gru_model_ibex35, test_loader_ibex35, scaler, -1, -1\n",
    ")\n",
    "\n",
    "print(\"GRU Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35_gru.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35_gru, y_pred_ibex35_gru, 'GRU Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. RNN Model Implementation\n",
    "\n",
    "Simple Recurrent Neural Network (RNN) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        \"\"\"\n",
    "        RNN model for time series forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        input_size (int): Number of features in the input\n",
    "        hidden_size (int): Number of features in the hidden state\n",
    "        num_layers (int): Number of recurrent layers\n",
    "        output_size (int): Number of features in the output\n",
    "        dropout (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN model for SP500\n",
    "rnn_model_sp500 = RNNModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model_sp500.parameters(), lr=0.001)\n",
    "\n",
    "rnn_model_sp500, train_losses_sp500_rnn, test_losses_sp500_rnn = train_model(\n",
    "    rnn_model_sp500, train_loader_sp500, test_loader_sp500, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RNN model for SP500\n",
    "y_true_sp500_rnn, y_pred_sp500_rnn, metrics_sp500_rnn = evaluate_model(\n",
    "    rnn_model_sp500, test_loader_sp500, scaler, -2, -2\n",
    ")\n",
    "\n",
    "print(\"RNN Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500_rnn.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500_rnn, y_pred_sp500_rnn, 'RNN Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN model for IBEX35\n",
    "rnn_model_ibex35 = RNNModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model_ibex35.parameters(), lr=0.001)\n",
    "\n",
    "rnn_model_ibex35, train_losses_ibex35_rnn, test_losses_ibex35_rnn = train_model(\n",
    "    rnn_model_ibex35, train_loader_ibex35, test_loader_ibex35, criterion, optimizer, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RNN model for IBEX35\n",
    "y_true_ibex35_rnn, y_pred_ibex35_rnn, metrics_ibex35_rnn = evaluate_model(\n",
    "    rnn_model_ibex35, test_loader_ibex35, scaler, -1, -1\n",
    ")\n",
    "\n",
    "print(\"RNN Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35_rnn.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35_rnn, y_pred_ibex35_rnn, 'RNN Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ensemble Model Implementation\n",
    "\n",
    "Create an ensemble model by combining the predictions from LSTM, GRU, and RNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create ensemble predictions\n",
    "def ensemble_predictions(models, test_loader, scaler, feature_idx, target_idx, weights=None):\n",
    "    \"\"\"\n",
    "    Create ensemble predictions by combining the predictions from multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    models (list): List of models\n",
    "    test_loader (torch.utils.data.DataLoader): Testing data loader\n",
    "    scaler (sklearn.preprocessing.MinMaxScaler): Scaler used to normalize the data\n",
    "    feature_idx (int): Index of the feature in the original data\n",
    "    target_idx (int): Index of the target in the original data\n",
    "    weights (list): List of weights for each model (default: equal weights)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (y_true, y_pred, metrics)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "    # Set equal weights if not provided\n",
    "    if weights is None:\n",
    "        weights = [1/len(models)] * len(models)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Get predictions from each model\n",
    "            batch_preds = []\n",
    "            for model in models:\n",
    "                batch_pred = model(X_batch).cpu().numpy()\n",
    "                batch_preds.append(batch_pred)\n",
    "            \n",
    "            # Combine predictions using weighted average\n",
    "            ensemble_pred = np.zeros_like(batch_preds[0])\n",
    "            for i, pred in enumerate(batch_preds):\n",
    "                ensemble_pred += weights[i] * pred\n",
    "            \n",
    "            # Move to CPU for numpy conversion\n",
    "            y_batch = y_batch.cpu().numpy()\n",
    "            \n",
    "            y_true.extend(y_batch)\n",
    "            y_pred.extend(ensemble_pred)\n",
    "    \n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    return y_true, y_pred, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions for SP500\n",
    "models_sp500 = [lstm_model_sp500, gru_model_sp500, rnn_model_sp500]\n",
    "\n",
    "# Use weights based on individual model performance (inverse of MSE)\n",
    "mse_values = [metrics_sp500['MSE'], metrics_sp500_gru['MSE'], metrics_sp500_rnn['MSE']]\n",
    "weights = [1/mse for mse in mse_values]\n",
    "weights = [w/sum(weights) for w in weights]  # Normalize weights\n",
    "\n",
    "print(f\"Ensemble weights for SP500: {weights}\")\n",
    "\n",
    "y_true_sp500_ensemble, y_pred_sp500_ensemble, metrics_sp500_ensemble = ensemble_predictions(\n",
    "    models_sp500, test_loader_sp500, scaler, -2, -2, weights=weights\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble Model Metrics for SP500:\")\n",
    "for metric, value in metrics_sp500_ensemble.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_sp500_ensemble, y_pred_sp500_ensemble, 'Ensemble Model Predictions for SP500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions for IBEX35\n",
    "models_ibex35 = [lstm_model_ibex35, gru_model_ibex35, rnn_model_ibex35]\n",
    "\n",
    "# Use weights based on individual model performance (inverse of MSE)\n",
    "mse_values = [metrics_ibex35['MSE'], metrics_ibex35_gru['MSE'], metrics_ibex35_rnn['MSE']]\n",
    "weights = [1/mse for mse in mse_values]\n",
    "weights = [w/sum(weights) for w in weights]  # Normalize weights\n",
    "\n",
    "print(f\"Ensemble weights for IBEX35: {weights}\")\n",
    "\n",
    "y_true_ibex35_ensemble, y_pred_ibex35_ensemble, metrics_ibex35_ensemble = ensemble_predictions(\n",
    "    models_ibex35, test_loader_ibex35, scaler, -1, -1, weights=weights\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble Model Metrics for IBEX35:\")\n",
    "for metric, value in metrics_ibex35_ensemble.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results(y_true_ibex35_ensemble, y_pred_ibex35_ensemble, 'Ensemble Model Predictions for IBEX35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance for SP500\n",
    "models_sp500_names = ['LSTM', 'GRU', 'RNN', 'Ensemble']\n",
    "metrics_list_sp500 = [metrics_sp500, metrics_sp500_gru, metrics_sp500_rnn, metrics_sp500_ensemble]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_sp500 = pd.DataFrame(index=models_sp500_names)\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R2']:\n",
    "    comparison_sp500[metric] = [metrics[metric] for metrics in metrics_list_sp500]\n",
    "\n",
    "print(\"Model Performance Comparison for SP500:\")\n",
    "print(comparison_sp500)\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['MSE', 'RMSE', 'MAE']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    plt.bar(models_sp500_names, comparison_sp500[metric])\n",
    "    plt.title(f'{metric} Comparison for SP500')\n",
    "    plt.ylabel(metric)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 separately (higher is better)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(models_sp500_names, comparison_sp500['R2'])\n",
    "plt.title('R2 Comparison for SP500')\n",
    "plt.ylabel('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance for IBEX35\n",
    "models_ibex35_names = ['LSTM', 'GRU', 'RNN', 'Ensemble']\n",
    "metrics_list_ibex35 = [metrics_ibex35, metrics_ibex35_gru, metrics_ibex35_rnn, metrics_ibex35_ensemble]\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_ibex35 = pd.DataFrame(index=models_ibex35_names)\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R2']:\n",
    "    comparison_ibex35[metric] = [metrics[metric] for metrics in metrics_list_ibex35]\n",
    "\n",
    "print(\"Model Performance Comparison for IBEX35:\")\n",
    "print(comparison_ibex35)\n",
    "\n",
    "# Plot the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['MSE', 'RMSE', 'MAE']):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    plt.bar(models_ibex35_names, comparison_ibex35[metric])\n",
    "    plt.title(f'{metric} Comparison for IBEX35')\n",
    "    plt.ylabel(metric)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 separately (higher is better)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(models_ibex35_names, comparison_ibex35['R2'])\n",
    "plt.title('R2 Comparison for IBEX35')\n",
    "plt.ylabel('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Models for Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for models if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Function to save a model\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a PyTorch model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Model to save\n",
    "    filename (str): Filename to save the model to\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# Save SP500 models\n",
    "save_model(lstm_model_sp500, 'models/lstm_sp500.pth')\n",
    "save_model(gru_model_sp500, 'models/gru_sp500.pth')\n",
    "save_model(rnn_model_sp500, 'models/rnn_sp500.pth')\n",
    "\n",
    "# Save IBEX35 models\n",
    "save_model(lstm_model_ibex35, 'models/lstm_ibex35.pth')\n",
    "save_model(gru_model_ibex35, 'models/gru_ibex35.pth')\n",
    "save_model(rnn_model_ibex35, 'models/rnn_ibex35.pth')\n",
    "\n",
    "# Save model parameters for later use\n",
    "model_params = {\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers,\n",
    "    'output_size': output_size,\n",
    "    'dropout': dropout,\n",
    "    'seq_length': seq_length,\n",
    "    'feature_columns': df.columns.tolist(),\n",
    "    'ensemble_weights_sp500': weights,\n",
    "    'ensemble_weights_ibex35': weights,\n",
    "    'target_idx_sp500': -2,\n",
    "    'target_idx_ibex35': -1\n",
    "}\n",
    "\n",
    "with open('models/model_params.pkl', 'wb') as f:\n",
    "    pickle.dump(model_params, f)\n",
    "print(\"Model parameters saved to models/model_params.pkl\")\n",
    "\n",
    "# Save scaler\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved to models/scaler.pkl\")\n",
    "\n",
    "# Save feature columns\n",
    "with open('models/feature_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(df.columns.tolist(), f)\n",
    "print(\"Feature columns saved to models/feature_columns.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "In this notebook, we have implemented LSTM, GRU, and RNN models to predict the future prices of SP500 and IBEX35 indices. We have enhanced our models with economic indicators, technical analysis features, and seasonality decomposition. We have also created an ensemble model by combining the predictions from these models. The models have been saved for later use in the Streamlit app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
